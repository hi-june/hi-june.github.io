---
title:  "[Team_ForV] AI Models"
excerpt: "Team_ForV의 프로젝트"

categories:
  - Team_ForV
tag:
  - AI

toc: true
toc_sticky: true

date: 2022-08-26
last_modified_at: 2022-08-28
---
## 음성 합성은 어떻게 이루어질까?
![image](/assets/images/Team_ForV/8-0.png){: width="90%" height="90%" .align-center}<br>
대부분의 음성 합성 네트워크는 두 단계로 이루어진다!  

1) Mel-spectrogram 생성  
2) waveform 생성

이 프로젝트에서는 `Glow-TTS`와 `HiFi-GAN`을 사용하여 위의 두 가지 단계를 수행하고, 그 결과물을 이용하여 음성 합성 과정을 수행한다.  

각각에 대해서 차근차근 알아보자!  

## Mel-spectrogram의 생성
학습에 필요한 음성 녹음을 마쳤다면, 녹음한 음성 데이터들로 목소리 학습을 진행해야 한다.  

음성 데이터를 raw data를 그대로 사용하면 <u>파라미터가 너무 많아지기도</u> 하고 <u>데이터 용량이 너무 커지므로</u>  
보통 `Mel-spectrogram`을 많이 사용한다고 한다.  

음성파일에서 의미있는 정보를 얻기 위해선 음성 데이터를 컴퓨터가 다루기 쉽도록 가공해주어야 하는데, 여기서부터 '신호처리'의 영역으로 들어간다.  

### - <u>신호(Signals)란?</u>
Signal은 시간에 따른 특정 양의 변화이다.  
Audio같은 경우 공기의 압력이 이에 해당하는데,  
이 정보를 디지털 방식으로 얻는 방법은 시간에 변화에 따른 기압 sample을 채취하는 것으로, 데이터를 샘플링하는 속도를 조절하여 얻을 수 있다.  
이렇게 얻어낸 파형 신호에 대해 컴퓨터로 해석하고, 분석 및 수정을 하는 것이다.  
아래 예시는 librosa 라이브러리를 이용하여 wav파일로부터 파형 신호를 얻어내는 예제이다.

![image](/assets/images/Team_ForV/8-1.png){: width="40%" height="40%"}<br>
```python
import librosa
import librosa.display
import matplotlib.pyplot as plt
y, sr = librosa.load('./example_data/blues.00000.wav')

plt.plot(y)
plt.title('Signal')
plt.xlabel('Time (samples)')
plt.ylabel('Amplitude')
```

이렇게 얻어낸 signal 자체만으로는 의미가 없고, 이로부터 유용한 정보를 뽑아내기 위해 '푸리에 변환'이라는 개념이 등장한다.

### - <u>푸리에 변환(Fourier Transform)이란?</u>
푸리에 변환을 간단히 말하자면 '입력 신호를 다양한 주파수를 가지는 주기함수들로 분해하는 것'이다.  

![image](/assets/images/Team_ForV/8-2.png){: width="40%" height="40%"}<br>

오디오 신호는 여러 개의 단일 주파수 음파들로 구성되는데,  
`time domain`에서는 진폭(amplitude)만 얻어낼 것을 '푸리에 변환'을 통해  
`frequency domain`에서 주파수(frequency)와 진폭(amplitude)를 가지는 주기함수를 얻어내는 것이다.  
이렇게 결과를 `spectrum`이라고 한다.  

### - <u>Fast Fourier Transform(FFT)</u>
푸리에 변환의 한 예로 fast Fourier transform (FFT)는 신호처리에서 널리 쓰이는 알고리즘인데,  
다음 예제를 보자.  

```python
import numpy as np
n_fft = 2048
ft = np.abs(librosa.stft(y[:n_fft], hop_length = n_fft+1))
plt.plot(ft)
plt.title('Spectrum')
plt.xlabel('Frequency Bin')
plt.ylabel('Amplitude')
```

![image](/assets/images/Team_ForV/8-3.png){: width="40%" height="40%"}<br>

자, 그런데 뭔가 좀 이상하다.  
FFT를 통해 얻은 spectrum에는 시간 정보가 없다!  
음악이나 speech같은 non periodic한 audio signal은 주파수 정보가 시간마다 달라지기 때문에 FFT만 가지고는 사용할 수는 없다.  

따라서, '음성 데이터를 시간 단위로 짧게 쪼개서 FFT를 해주자!'라는 해결책이 나왔고,  
이것이 Short Time Fourier Transform이다.  

### - <u>Short Time Fourier Transform(STFT) & Spectrogram</u>
STFT를 통해 나오는 spectrogram은 서로의 위에 쌓인 FFT들의 묶음으로 생각할 수 있다.  
각각 다른 주파수에 대해 시간이 지남에 따라 달라지기 때문에, spectrogram은 신호의 크기 또는 진폭을 시각적으로 보여줄 수 있다.  

```python
spec = np.abs(librosa.stft(y, hop_length=512))
spec = librosa.amplitude_to_db(spec, ref=np.max)

librosa.display.specshow(spec, sr=sr, x_axis='time', y_axis='log')
plt.colorbar(format='%+2.0f dB')
plt.title('Spectrogram')
```
![image](/assets/images/Team_ForV/8-4.png){: width="50%" height="50%"}<br>

자, 먼 길 왔는데, 그래서 Mel-Spectrogram은 뭐냐..?  

### - <u>Mel-Spectrogram</u>
사람들은 음성 신호를 인식할 때 주파수를 linear scale로 인식하는게 아니라고 한다.  
또, 낮은 주파수를 높은 주파수보다 더 예민하게 받아들이는데, 예를 들어 500 ~ 1000 Hz 가 바뀌는건 예민하게 인식하는데 10000Hz ~ 20000 Hz가 바뀌는 것은 잘 인식 못한다는 것이다.  

![image](/assets/images/Team_ForV/8-5.gif){: width="30%" height="30%"}<br>

동일한 pitch distance면 듣는 사람으로 하여금 동일한 정도로 멀리 들리도록하는 pitch 단위를 제안했는데, 이를 mel scale이라고 하고, 이 주파수를 mel scale로 볼 수 있게 한 것이다.

앞선 spectrogram에 mel scale을 적용하면 Mel-Spectrogram을 만들 수 있다.  

```python
mel_spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)
mel_spect = librosa.power_to_db(spect, ref=np.max)
librosa.display.specshow(mel_spect, y_axis='mel', fmax=8000, x_axis='time')
plt.title('Mel Spectrogram')
plt.colorbar(format='%+2.0f dB')
```
![image](/assets/images/Team_ForV/8-6.png){: width="50%" height="50%"}<br>

## wave form의 생성


## 다음 글 소개

## Reference
<https://sce-tts.github.io/#/v2/train> [TTS 모델학습]
<https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53> [Understanding the Mel Spectrogram]<br>
